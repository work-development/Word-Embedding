# Word Embedding   

<p align="justify">
Будем работать с англоязычным датасетом, составленным из рецептов 
</p>

## Модель     

* word2vec (SkipGram)     
* Softmax (с аппроксимацией Negative Sampling)

<p align="justify">
Моделируем условное распределение соседних слов в некотором окне при условии того, что мы пронаблюдали центральное слово. Разложим распределение на произведения более простых распределений - насколько вероятно можно встретить какое-то контекстное слово рядом с центральным словом (обучение word2vec сводится к обучению классификатора, который предсказывает — могут ли два слова встретиться в рамках какого-то небольшого окна, или не могут). В этом распределении есть две категориальные случайные величины.  Моделировать эти категориальные распределения будим с помощью Softmax в который подаём оценки сходства слов. Для оптимизации Softmax заменим сумму по всему словарю в знаменателе суммой по небольшому количеству случайно выбранных слов и, каждый раз, когда нам нужно посчитать аппроксимацию к софтмаксу, будим выбирать новые случайные слова (negative sampling). Сходство слов мы будем моделировать как скалярное произведение векторов этих слов. В моделе SkipGram для каждого слова у нас есть два вектора — первый вектор используется, когда слово находится в центре скользящего окна, и второй вектор используется, когда это слово описывает контекст (т.о. есть две матрицы проинициализированные равномерным шумом). Настраивать значения этих матриц будим методом бинарной кросс-энтропии. С помощью скалярного произведения мы оцениваем семантическое сходство слов из настоящих предложений и случайно выбранных отрицательных слов. Далее мы также используем кросс-энтропию, для того чтобы сходство настоящих слов и отрицательных слов было поменьше, то есть мы передаём нули как таргеты. В результате метод возвращает сумму функции потерь для положительных примеров и для отрицательных.
</p>

##  Универсальные компоненты     

* Токенизация   
* Построение словаря   
* pytorch Dataset    
* Паддинги
* Бинарная кросс-энтропия

## Основные функции и классы   


<b>PaddedSequenceDataset</b> - класс c 2-мя методами:   
    1-ый метод определяет сколько предложений в нём есть;   
    2-ой метод возвращения предложение по номеру, так что если предложение короче некоторой заданной длины, то он добавляет нули в конец этого предложения, если предложение длиннее установленного порога, то он его обрезает. Эта функция возвращает пары — а именно "текст" и "какая-то метка", которую по этому тексту нужно предсказывать (если задача классификации).    

<p align="justify">
<b>SkipGramNegativeSamplingTrainer</b> - класс, содержащий метод forward (алгоритм обучения), который вычисляет функцию потерь. На вход на каждой итерации подается пачка предложений — прямоугольный тензор размерности ["размер батча" на "максимальную длину предложения"]. На первом шаге, для каждого токена из батча, мы получаем центральный эмбеддинг, то есть делаем выборку из таблицы центральных эмбеддингов. В результате получаем тензор размером ["размер батча" на "максимально длину предложения",  "размер эмбеддинга"]. Далее бурутся все токены, которые у нас есть в батче, и делаем для них выборку из таблицы контекстных токенов. Затем производится матричное произведение для каждой пары матриц в батче. В результате получается тензор (оценка сходства,  для каждой пары токенов, встретившихся внутри предложения) размером ["количество примеров в батче" на "максимальную длину предложения" на "максимальную длину предложения"]. Далее мы преобразовываем оценки сходства в вероятности с помощью сигмоиды, затем мы домножаем тензор вероятностей, который содержит вероятность для всех возможных пар токенов из наших предложений, на маску (т.е. зануляем оценки вероятностей для всех пар токенов, которые не лежат внутри окна). Далее мы говорим, что настоящая метка для этих позиций — это единичка. То есть, для этих пар слов и этих позиций, модель должна предсказывать большую вероятность совместной встречаемости. Для обучения используем бинарную кросс-энтропию. Для отрицательных примеров алгоритм похож. С помощью "torch.randint" получаем номера токенов, которые мы будем использовать в качестве отрицательных.  Далее мы делаем выборку из таблицы контекстных векторов для полученных отрицательных токенов и сразу же её транспонируем. Получаем тензор размером ["количество предложений в батче" на "размер эмбеддинга" и на "количество отрицательных слов"]. В итоге метод возвращает сумму функции потерь для положительных примеров и для отрицательных.
</p>



## Технологии
* [PyTorch](https://pytorch.org/)   
* [pandas](https://pandas.pydata.org/)
* [numpy](https://numpy.org/)
* [matplotlib](https://matplotlib.org/)
